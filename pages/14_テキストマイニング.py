import os
from collections import Counter

import japanize_matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import networkx as nx
import nlplot
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from PIL import Image
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud
try:
    from networkx.algorithms import community
except ImportError:
    community = None

import common


common.set_font()

# ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã‚’å–å¾—ï¼ˆmatplotlibã®è¨­å®šã‚’æ´»ç”¨ï¼‰
def get_japanese_font_path():
    """matplotlibã§è¨­å®šã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’å–å¾—"""
    try:
        # IPAexGothicãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œç´¢
        japanese_fonts = [f for f in fm.fontManager.ttflist
                         if 'IPA' in f.name or 'Noto Sans CJK' in f.name
                         or 'Takao' in f.name]

        if japanese_fonts:
            # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
            return japanese_fonts[0].fname

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œç´¢
        font_candidates = [
            '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',
            '/usr/share/fonts/truetype/fonts-japanese-gothic.ttf',
            '/usr/share/fonts/truetype/takao-gothic/TakaoGothic.ttf',
        ]

        for candidate in font_candidates:
            if os.path.exists(candidate):
                return candidate

        return None
    except Exception as e:
        st.warning(f"ãƒ•ã‚©ãƒ³ãƒˆæ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

font_path = get_japanese_font_path()
if font_path is None:
    st.warning("âš ï¸ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã§æ–‡å­—åŒ–ã‘ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

st.set_page_config(page_title="ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°", layout="wide")

# AIè§£é‡ˆæ©Ÿèƒ½ã®è¨­å®š
gemini_api_key, enable_ai_interpretation = common.AIStatisticalInterpreter.setup_ai_sidebar()

st.title("ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°")
common.display_header()
st.write(
    "è¨˜è¿°å¤‰æ•°ã‹ã‚‰ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚„å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æŠ½å‡ºã—ã¾ã™ã€‚"
    "KH Coderã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«åŸºã¥ã„ã¦ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºï¼‰ã•ã‚Œã¦è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"
)

# ç”»åƒã®è¡¨ç¤º
try:
    image = Image.open('images/textmining.png')
    st.image(image)
except FileNotFoundError:
    pass

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿é¸æŠ
uploaded_file = st.file_uploader("CSVã¾ãŸã¯Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", type=["csv", "xlsx"])
use_demo_data = st.checkbox('ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨')

df = None
if use_demo_data:
    try:
        df = pd.read_excel('datasets/textmining_demo.xlsx', sheet_name=0)
        st.write("ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿:")
        st.write(df.head())
    except FileNotFoundError:
        st.error("ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« 'textmining_demo.xlsx' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
else:
    if uploaded_file is not None:
        try:
            if uploaded_file.type == 'text/csv':
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            st.write("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿:")
            st.write(df.head())
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


def create_cooccurrence_network_with_communities(graph, title='å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯',
                                                  top_n_edges=60, use_plotly=True,
                                                  node_to_word=None):
    """
    KH Coderã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«åŸºã¥ã„ãŸå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”»
    ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«è‰²åˆ†ã‘

    Parameters:
    -----------
    graph : networkx.Graph
        å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ï¼ˆãƒãƒ¼ãƒ‰ã¯æ•°å€¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
    title : str
        ã‚°ãƒ©ãƒ•ã®ã‚¿ã‚¤ãƒˆãƒ«
    top_n_edges : int
        è¡¨ç¤ºã™ã‚‹ä¸Šä½ã‚¨ãƒƒã‚¸æ•°ï¼ˆKH Coderã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯60ï¼‰
    use_plotly : bool
        Plotlyã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆFalseã®å ´åˆã¯matplotlibï¼‰
    node_to_word : dict or None
        ãƒãƒ¼ãƒ‰ç•ªå·ã‹ã‚‰å˜èªã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸
        Noneã®å ´åˆã¯ãƒãƒ¼ãƒ‰ç•ªå·ã‚’ãã®ã¾ã¾ä½¿ç”¨
    """

    # ãƒãƒ¼ãƒ‰ç•ªå·ã‹ã‚‰å˜èªã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆãªã„å ´åˆã¯ãƒãƒ¼ãƒ‰ç•ªå·ã‚’ä½¿ç”¨ï¼‰
    if node_to_word is None:
        node_to_word = {node: str(node) for node in graph.nodes()}

    if graph is None or len(graph.edges()) == 0:
        st.warning("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return None

    # KH Coderã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : Top N ã‚¨ãƒƒã‚¸ã‚’æŠ½å‡º
    edges_sorted = sorted(graph.edges(data=True),
                         key=lambda x: x[2].get('weight', 1),
                         reverse=True)[:top_n_edges]

    # ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
    subgraph = nx.Graph()
    for u, v, data in edges_sorted:
        subgraph.add_edge(u, v, weight=data.get('weight', 1))

    if len(subgraph.nodes()) == 0:
        st.warning("è¡¨ç¤ºå¯èƒ½ãªãƒãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return None

    # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºï¼ˆKH Coderã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
    if community is not None:
        try:
            # Louvainã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚’æ¤œå‡º
            communities = community.greedy_modularity_communities(subgraph)

            # ãƒãƒ¼ãƒ‰ã«ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£IDã‚’å‰²ã‚Šå½“ã¦
            node_to_community = {}
            for idx, comm in enumerate(communities):
                for node in comm:
                    node_to_community[node] = idx
        except Exception as e:
            st.warning(f"ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã™ã¹ã¦åŒã˜ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
            node_to_community = {node: 0 for node in subgraph.nodes()}
    else:
        # networkx.communityãŒåˆ©ç”¨ã§ããªã„å ´åˆ
        node_to_community = {node: 0 for node in subgraph.nodes()}

    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—ï¼ˆKH Coderã¯Kamada-Kawaiã‚’ä½¿ç”¨ï¼‰
    try:
        pos = nx.kamada_kawai_layout(subgraph)
    except:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: spring layout
        pos = nx.spring_layout(subgraph, k=1, iterations=50)

    # ãƒãƒ¼ãƒ‰ã®ä¸­å¿ƒæ€§ã‚’è¨ˆç®—ï¼ˆãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚ºç”¨ï¼‰
    try:
        degree_centrality = nx.degree_centrality(subgraph)
    except:
        degree_centrality = {node: 1 for node in subgraph.nodes()}

    if use_plotly:
        # Plotlyã§æç”»
        edge_trace = []

        # ã‚¨ãƒƒã‚¸ã‚’æç”»
        for edge in subgraph.edges(data=True):
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            weight = edge[2].get('weight', 1)

            edge_trace.append(
                go.Scatter(
                    x=[x0, x1, None],
                    y=[y0, y1, None],
                    mode='lines',
                    line=dict(width=0.5 + weight * 0.5, color='#888'),
                    hoverinfo='none',
                    showlegend=False
                )
            )

        # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã”ã¨ã«è‰²ã‚’è¨­å®š
        num_communities = len(set(node_to_community.values()))
        colors = px.colors.qualitative.Set3[:num_communities] if num_communities <= len(px.colors.qualitative.Set3) else px.colors.sample_colorscale("turbo", [n/(num_communities-1) for n in range(num_communities)])

        # ãƒãƒ¼ãƒ‰ã‚’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã”ã¨ã«æç”»
        node_traces = []
        for comm_id in set(node_to_community.values()):
            nodes_in_comm = [node for node, c in node_to_community.items() if c == comm_id]

            node_x = []
            node_y = []
            node_text = []
            node_size = []

            for node in nodes_in_comm:
                x, y = pos[node]
                node_x.append(x)
                node_y.append(y)
                node_text.append(f"{node_to_word.get(node, str(node))}<br>ã‚°ãƒ«ãƒ¼ãƒ—: {comm_id + 1}<br>ä¸­å¿ƒæ€§: {degree_centrality[node]:.3f}")
                node_size.append(20 + degree_centrality[node] * 100)

            node_trace = go.Scatter(
                x=node_x,
                y=node_y,
                mode='markers+text',
                text=[node_to_word.get(node, str(node)) for node in nodes_in_comm],
                textposition='middle center',
                textfont=dict(
                    size=12,
                    family='IPAexGothic, "Hiragino Sans", "Noto Sans CJK JP", "Yu Gothic", Meiryo, Arial, sans-serif',
                    color='black'
                ),
                hovertext=node_text,
                hoverinfo='text',
                marker=dict(
                    size=node_size,
                    color=colors[comm_id % len(colors)],
                    line=dict(width=2, color='white')
                ),
                name=f'ã‚°ãƒ«ãƒ¼ãƒ— {comm_id + 1}',
                showlegend=True
            )
            node_traces.append(node_trace)

        # å›³ã‚’ä½œæˆ
        fig = go.Figure(data=edge_trace + node_traces)

        fig.update_layout(
            title=dict(text=title, x=0.5, xanchor='center'),
            showlegend=True,
            hovermode='closest',
            margin=dict(b=0, l=0, r=0, t=40),
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            plot_bgcolor='white',
            width=1000,
            height=600,
            font=dict(
                family='IPAexGothic, "Hiragino Sans", "Noto Sans CJK JP", "Yu Gothic", Meiryo, Arial, sans-serif',
                size=12,
                color='black'
            )
        )

        return fig
    else:
        # matplotlibã§æç”»
        fig_net, ax = plt.subplots(figsize=(12, 8))

        # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã”ã¨ã«è‰²ã‚’è¨­å®š
        num_communities = len(set(node_to_community.values()))
        cmap = plt.cm.get_cmap('Set3', num_communities)
        node_colors = [cmap(node_to_community[node]) for node in subgraph.nodes()]

        # ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã‚’ä¸­å¿ƒæ€§ã«åŸºã¥ã„ã¦è¨­å®š
        node_sizes = [300 + degree_centrality[node] * 2000 for node in subgraph.nodes()]

        # ã‚¨ãƒƒã‚¸ã®å¤ªã•ã‚’é‡ã¿ã«åŸºã¥ã„ã¦è¨­å®š
        edge_weights = [subgraph[u][v].get('weight', 1) for u, v in subgraph.edges()]
        edge_widths = [0.5 + w * 0.5 for w in edge_weights]

        # æç”»
        nx.draw_networkx_edges(subgraph, pos, width=edge_widths, alpha=0.5, ax=ax)
        nx.draw_networkx_nodes(subgraph, pos, node_color=node_colors,
                              node_size=node_sizes, alpha=0.9, ax=ax)
        # ãƒãƒ¼ãƒ‰ç•ªå·ã‚’å˜èªã«å¤‰æ›ã—ãŸãƒ©ãƒ™ãƒ«ã‚’ä½œæˆ
        labels = {node: node_to_word.get(node, str(node)) for node in subgraph.nodes()}
        nx.draw_networkx_labels(subgraph, pos, labels=labels, font_family='IPAexGothic',
                               font_size=12, font_weight='bold', ax=ax)

        ax.set_title(title, fontsize=14, pad=20)
        ax.axis('off')
        plt.tight_layout()

        return fig_net


# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿è§£æé–‹å§‹
if df is not None and not df.empty:
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    text_cols = df.select_dtypes(include=['object']).columns.tolist()

    if not categorical_cols:
        st.error('ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚')
    elif not text_cols:
        st.error('è¨˜è¿°å¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚')
    else:
        st.subheader("ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®é¸æŠ")
        selected_category = st.selectbox('ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„', categorical_cols)
        if selected_category in text_cols:
            text_cols.remove(selected_category)

        default_index = len(text_cols) - 1 if text_cols else 0
        selected_text = st.selectbox('è¨˜è¿°å¤‰æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„', text_cols, index=default_index)

        st.subheader('å…¨ä½“ã®åˆ†æ')
        tokenizer = Tokenizer()

        def extract_words(text):
            if pd.isnull(text):
                return ""
            tokens = tokenizer.tokenize(text)
            return ' '.join(
                token.base_form
                for token in tokens
                if token.part_of_speech.split(',')[0] in ["åè©", "å‹•è©", "å½¢å®¹è©", "å‰¯è©"]
            )

        df['tokenized_text'] = df[selected_text].apply(extract_words)
        total_tokens = df['tokenized_text'].str.split().apply(len).sum()
        st.write(f"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å¾Œã®ç·å˜èªæ•°: {total_tokens}")

        # å…±èµ·ãŒç™ºç”Ÿã—ãªã„å ´åˆã®ãƒ†ã‚¹ãƒˆè¡Œè¿½åŠ 
        if not df['tokenized_text'].str.strip().any():
            df = pd.concat([
                df,
                pd.DataFrame({
                    selected_category: ["ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒª"],
                    selected_text: ["ãƒ†ã‚¹ãƒˆ ãƒ†ã‚¹ãƒˆ ãƒ†ã‚­ã‚¹ãƒˆ ãƒ†ã‚­ã‚¹ãƒˆ ãƒ‡ãƒ¼ã‚¿ ãƒ‡ãƒ¼ã‚¿ åˆ†æ åˆ†æ"]
                })
            ], ignore_index=True)
            df['tokenized_text'] = df[selected_text].apply(extract_words)

        # NLPlot åˆæœŸåŒ–
        npt = nlplot.NLPlot(df, target_col='tokenized_text')
        stopwords_list = npt.get_stopword()
        words = ' '.join(df['tokenized_text'])

        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆKH Coderã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
        st.subheader('ã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€‘')
        max_words = st.slider(
            'æœ€å¤§å˜èªæ•°', 10, max(len(set(words.split())), 10), 50
        )
        if words and font_path:
            try:
                wc = WordCloud(
                    width=800,
                    height=400,
                    max_words=max_words,
                    background_color='white',
                    font_path=font_path,
                    collocations=False,
                    stopwords=set(stopwords_list),
                    relative_scaling=0.5,  # KH Coderã‚¹ã‚¿ã‚¤ãƒ«
                    min_font_size=10
                ).generate(words)

                fig_wc, ax_wc = plt.subplots(figsize=(10, 5))
                ax_wc.imshow(wc, interpolation='bilinear')
                ax_wc.axis('off')
                st.pyplot(fig_wc)
                plt.close(fig_wc)
            except Exception as e:
                st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        elif not font_path:
            st.error("âš ï¸ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
        else:
            st.warning("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")

        # å…¨ä½“ã®å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆKH Coderã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
        st.subheader('ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆå…¨ä½“ï¼‰ã€‘')
        st.write("ğŸ’¡ ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«è‰²åˆ†ã‘ã•ã‚Œã¦ã„ã¾ã™ï¼ˆKH Coderã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰")

        try:
            npt.build_graph(stopwords=stopwords_list, min_edge_frequency=1)
            
            # nlplotã®ã‚°ãƒ©ãƒ•ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã£ã¦å±æ€§åãŒç•°ãªã‚‹ï¼‰
            # nlplotã®ã‚°ãƒ©ãƒ•ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚„å®Ÿè£…ã«ã‚ˆã£ã¦å±æ€§åãŒç•°ãªã‚‹ï¼‰
            graph_obj = getattr(npt, 'nwx', None) or getattr(npt, 'G', None) or getattr(npt, 'graph', None)
            
            if graph_obj is None:
                st.error("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚°ãƒ©ãƒ•ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                raise AttributeError("NLPlot object has no valid graph attribute (tried: nwx, G, graph)")

            # nlplotã®node_dfã‹ã‚‰ãƒãƒ¼ãƒ‰ç•ªå·ã¨å˜èªã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
            node_to_word_mapping = None
            if hasattr(npt, "node_df"):
                try:
                    node_df = npt.node_df
                    if node_df is not None:
                        # ãƒ‡ãƒãƒƒã‚°: node_dfã®æ§‹é€ ã‚’ç¢ºèª
                        st.write("ğŸ” ãƒ‡ãƒãƒƒã‚°: node_dfã®æƒ…å ±")
                        st.write(f"  - åˆ—: {list(node_df.columns)}")
                        st.write(f"  - è¡Œæ•°: {len(node_df)}")
                        st.write(f"  - æœ€åˆã®3è¡Œ:")
                        st.write(node_df.head(3))
                        
                        # ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
                        if "word" in node_df.columns:
                            # nodeåˆ—ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°indexã‚’ä½¿ç”¨
                            if "node" in node_df.columns:
                                node_to_word_mapping = dict(zip(node_df["node"], node_df["word"]))
                            else:
                                node_to_word_mapping = node_df["word"].to_dict()
                            
                            st.write(f"  - ãƒãƒƒãƒ”ãƒ³ã‚°ä¾‹: {dict(list(node_to_word_mapping.items())[:5])}")
                        elif "words" in node_df.columns:
                            if "node" in node_df.columns:
                                node_to_word_mapping = dict(zip(node_df["node"], node_df["words"]))
                            else:
                                node_to_word_mapping = node_df["words"].to_dict()
                except Exception as e:
                    st.error(f"ãƒãƒ¼ãƒ‰ã¨å˜èªã®ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆã§ã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    st.code(traceback.format_exc())

            # Plotlyãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è©¦è¡Œ
            fig_net = create_cooccurrence_network_with_communities(
                graph_obj,
                title='å…¨ä½“ã®å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰',
                top_n_edges=60,
                use_plotly=True,
                node_to_word=node_to_word_mapping
            )

            if fig_net is not None:
                st.plotly_chart(fig_net, use_container_width=True)
            else:
                # matplotlibãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                fig_net_mpl = create_cooccurrence_network_with_communities(
                    graph_obj,
                    title='å…¨ä½“ã®å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰',
                    top_n_edges=60,
                    use_plotly=False,
                    node_to_word=node_to_word_mapping
                )
                if fig_net_mpl is not None:
                    st.pyplot(fig_net_mpl)
                    plt.close(fig_net_mpl)
        except Exception as e:
            st.error(f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        # å˜èªåº¦æ•°ãƒãƒ¼
        freq = Counter(words.split())
        df_freq = pd.DataFrame(
            freq.items(), columns=['å˜èª','åº¦æ•°']
        ).sort_values(by='åº¦æ•°', ascending=False)
        if not df_freq.empty:
            fig_bar = px.bar(
                df_freq.head(20), x='å˜èª', y='åº¦æ•°',
                title='å‡ºç¾åº¦æ•°ãƒˆãƒƒãƒ—20'
            )
            st.plotly_chart(fig_bar)

        # --- AIè§£é‡ˆæ©Ÿèƒ½ ---
        if enable_ai_interpretation and gemini_api_key:
            try:
                # top_wordsã‚’ãƒªã‚¹ãƒˆå½¢å¼ã§å–å¾—
                top_words = [(row["å˜èª"], row["åº¦æ•°"]) for _, row in df_freq.head(30).iterrows()]
                n_documents = len(df)
                n_unique_words = len(freq)

                text_results = {
                    'top_words': top_words,
                    'n_documents': n_documents,
                    'n_unique_words': n_unique_words
                }

                common.AIStatisticalInterpreter.display_ai_interpretation(
                    api_key=gemini_api_key,
                    enabled=enable_ai_interpretation,
                    results=text_results,
                    analysis_type='text_mining',
                    key_prefix='text_mining'
                )
            except Exception as e:
                st.warning(f"AIè§£é‡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æã¨æç”»
        for cat, grp in df.groupby(selected_category):
            st.subheader(f'ï¼œã‚«ãƒ†ã‚´ãƒªï¼š{cat}ï¼')
            grp = grp.copy()
            grp['tokenized_text'] = grp[selected_text].apply(extract_words)
            words_cat = ' '.join(grp['tokenized_text'])

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
            if words_cat and font_path:
                try:
                    wc_cat = WordCloud(
                        width=600,
                        height=300,
                        max_words=50,
                        background_color='white',
                        font_path=font_path,
                        collocations=False,
                        stopwords=set(stopwords_list),
                        relative_scaling=0.5,
                        min_font_size=10
                    ).generate(words_cat)

                    fig_c, ax_c = plt.subplots(figsize=(8, 4))
                    ax_c.imshow(wc_cat, interpolation='bilinear')
                    ax_c.axis('off')
                    st.pyplot(fig_c)
                    plt.close(fig_c)
                except Exception as e:
                    st.warning(f"ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆKH Coderã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
            try:
                npt_cat = nlplot.NLPlot(grp, target_col='tokenized_text')
                npt_cat.build_graph(stopwords=stopwords_list, min_edge_frequency=1)
                
                # nlplotã®ã‚°ãƒ©ãƒ•ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã£ã¦å±æ€§åãŒç•°ãªã‚‹ï¼‰
                # nlplotã®ã‚°ãƒ©ãƒ•ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚„å®Ÿè£…ã«ã‚ˆã£ã¦å±æ€§åãŒç•°ãªã‚‹ï¼‰
                graph_obj_cat = getattr(npt_cat, 'nwx', None) or getattr(npt_cat, 'G', None) or getattr(npt_cat, 'graph', None)
                
                if graph_obj_cat is None:
                    st.warning(f"ã‚«ãƒ†ã‚´ãƒªã€Œ{cat}ã€ã®å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚°ãƒ©ãƒ•ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    continue

                # nlplotã®node_dfã‹ã‚‰ãƒãƒ¼ãƒ‰ç•ªå·ã¨å˜èªã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
                node_to_word_mapping_cat = None
                if hasattr(npt_cat, "node_df"):
                    try:
                        node_df_cat = npt_cat.node_df
                        if node_df_cat is not None:
                            # ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
                            if "word" in node_df_cat.columns:
                                # nodeåˆ—ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°indexã‚’ä½¿ç”¨
                                if "node" in node_df_cat.columns:
                                    node_to_word_mapping_cat = dict(zip(node_df_cat["node"], node_df_cat["word"]))
                                else:
                                    node_to_word_mapping_cat = node_df_cat["word"].to_dict()
                            elif "words" in node_df_cat.columns:
                                if "node" in node_df_cat.columns:
                                    node_to_word_mapping_cat = dict(zip(node_df_cat["node"], node_df_cat["words"]))
                                else:
                                    node_to_word_mapping_cat = node_df_cat["words"].to_dict()
                    except Exception as e:
                        st.error(f"ã‚«ãƒ†ã‚´ãƒªã€Œ{cat}ã€ã®ãƒãƒ¼ãƒ‰-å˜èªãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆã§ã‚¨ãƒ©ãƒ¼: {e}")
                        import traceback
                        st.code(traceback.format_exc())

                fig_cat = create_cooccurrence_network_with_communities(
                    graph_obj_cat,
                    title=f'{cat}ã®å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰',
                    top_n_edges=60,
                    use_plotly=True,
                    node_to_word=node_to_word_mapping_cat
                )

                if fig_cat is not None:
                    st.plotly_chart(fig_cat, use_container_width=True)
                else:
                    # matplotlibã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    fig_cat_mpl = create_cooccurrence_network_with_communities(
                        graph_obj_cat,
                        title=f'{cat}ã®å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰',
                        top_n_edges=60,
                        use_plotly=False,
                        node_to_word=node_to_word_mapping_cat
                    )
                    if fig_cat_mpl is not None:
                        st.pyplot(fig_cat_mpl)
                        plt.close(fig_cat_mpl)
            except Exception as e:
                st.warning(f"ã‚«ãƒ†ã‚´ãƒªåˆ¥å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ•ãƒƒã‚¿ãƒ¼
common.display_copyright()
common.display_special_thanks()
